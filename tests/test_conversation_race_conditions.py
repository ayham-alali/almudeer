"""
Comprehensive tests for conversation-related race conditions.
Tests multi-device sync, presence tracking, and concurrent modifications.
"""

import asyncio
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from datetime import datetime, timedelta


class TestPresenceTrackingRaceConditions:
    """Test WebSocket presence tracking under concurrent connections"""

    @pytest.mark.asyncio
    async def test_multi_device_simultaneous_connect(self):
        """
        Test that presence doesn't flicker when multiple devices connect simultaneously.
        Reproduces issue #2.1 from audit report.
        """
        from services.websocket_manager import ConnectionManager, RedisPubSubManager
        
        manager = ConnectionManager()
        
        # Mock Redis
        mock_redis = AsyncMock()
        mock_redis.incr = AsyncMock(side_effect=[1, 2, 3])  # 3 devices connecting
        mock_redis.expire = AsyncMock()
        mock_redis.set = AsyncMock(return_value=True)  # First device gets the lock
        mock_redis.delete = AsyncMock()
        
        manager._pubsub = MagicMock()
        manager._pubsub.is_available = True
        manager._pubsub._redis_client = mock_redis
        
        # Mock WebSocket connections
        mock_ws_1 = AsyncMock()
        mock_ws_2 = AsyncMock()
        mock_ws_3 = AsyncMock()
        
        license_id = 123
        
        # Simulate simultaneous connections
        with patch("services.websocket_manager.broadcast_presence_update") as mock_broadcast:
            await asyncio.gather(
                manager.connect(mock_ws_1, license_id),
                manager.connect(mock_ws_2, license_id),
                manager.connect(mock_ws_3, license_id),
            )
            
            # Should only broadcast once (first device)
            assert mock_broadcast.call_count == 1
            mock_broadcast.assert_called_with(license_id, is_online=True)

    @pytest.mark.asyncio
    async def test_presence_flag_cleanup_on_disconnect(self):
        """
        Test that presence flag is properly cleaned up when last device disconnects.
        """
        from services.websocket_manager import ConnectionManager
        
        manager = ConnectionManager()
        mock_redis = AsyncMock()
        mock_redis.decr = AsyncMock(return_value=0)  # Last device disconnecting
        mock_redis.delete = AsyncMock()
        
        manager._pubsub = MagicMock()
        manager._pubsub.is_available = True
        manager._pubsub._redis_client = mock_redis
        
        mock_ws = AsyncMock()
        license_id = 123
        
        manager._connections[license_id] = {mock_ws}
        
        with patch("services.websocket_manager.broadcast_presence_update") as mock_broadcast:
            await manager.disconnect(mock_ws, license_id)
            
            # Should delete the presence flag
            mock_redis.delete.assert_called()
            call_args = mock_redis.delete.call_args[0][0]
            assert "presence:broadcast" in call_args
            
            # Should broadcast offline
            mock_broadcast.assert_called_with(license_id, is_online=False)


class TestConversationStateRaceConditions:
    """Test conversation state updates under concurrent modifications"""

    @pytest.mark.asyncio
    async def test_concurrent_conversation_updates(self):
        """
        Test that distributed lock prevents race conditions in conversation state.
        Reproduces issue #2.4 from audit report.
        """
        from models.inbox import upsert_conversation_state
        from services.distributed_lock import DistributedLock
        
        license_id = 123
        sender_contact = "test_user"
        
        # Mock lock acquisition
        mock_lock = AsyncMock()
        mock_lock.acquire = AsyncMock(return_value=True)
        mock_lock.release = AsyncMock()
        
        with patch("services.distributed_lock.DistributedLock") as MockLock:
            MockLock.return_value = mock_lock
            
            # Simulate concurrent updates
            await asyncio.gather(
                upsert_conversation_state(license_id, sender_contact),
                upsert_conversation_state(license_id, sender_contact),
                upsert_conversation_state(license_id, sender_contact),
            )
            
            # Lock should be acquired 3 times (once per call)
            assert mock_lock.acquire.call_count == 3
            assert mock_lock.release.call_count == 3

    @pytest.mark.asyncio
    async def test_lock_failure_graceful_degradation(self):
        """
        Test that conversation state update continues (degraded) if lock fails.
        """
        from models.inbox import upsert_conversation_state
        
        mock_lock = AsyncMock()
        mock_lock.acquire = AsyncMock(return_value=False)  # Lock acquisition fails
        
        with patch("services.distributed_lock.DistributedLock") as MockLock:
            MockLock.return_value = mock_lock
            
            # Should not raise exception
            await upsert_conversation_state(123, "test_user")
            
            # Should still attempt DB operations (graceful degradation)


class TestMessageDeduplication:
    """Test message deduplication logic"""

    @pytest.mark.asyncio
    async def test_telegram_message_deduplication(self):
        """
        Test that duplicate Telegram messages are properly filtered.
        Reproduces issue #2.2 from audit report.
        """
        from services.websocket_manager import RedisPubSubManager
        
        mock_redis = AsyncMock()
        mock_redis.exists = AsyncMock(return_value=1)  # Message already processed
        mock_redis.setex = AsyncMock()
        
        redis_mgr = RedisPubSubManager()
        redis_mgr._redis_client = mock_redis
        redis_mgr._initialized = True
        
        license_id = 123
        message_id = "999"
        
        # Check if message was processed
        processed_key = f"almudeer:telegram:processed:{license_id}:{message_id}"
        already_processed = await mock_redis.exists(processed_key)
        
        assert already_processed
        mock_redis.exists.assert_called_with(processed_key)

    @pytest.mark.asyncio
    async def test_deduplication_ttl(self):
        """
        Test that deduplication keys have proper TTL (24 hours).
        """
        from services.websocket_manager import RedisPubSubManager
        
        mock_redis = AsyncMock()
        mock_redis.exists = AsyncMock(return_value=0)  # New message
        mock_redis.setex = AsyncMock()
        
        redis_mgr = RedisPubSubManager()
        redis_mgr._redis_client = mock_redis
        redis_mgr._initialized = True
        
        license_id = 123
        message_id = "1000"
        
        processed_key = f"almudeer:telegram:processed:{license_id}:{message_id}"
        
        # Mark as processed
        await mock_redis.setex(processed_key, 86400, "1")
        
        # Should set 24-hour TTL
        mock_redis.setex.assert_called_with(processed_key, 86400, "1")


class TestTypingIndicatorMemoryLeak:
    """Test typing indicator timer cleanup"""

    def test_typing_timer_cleanup_on_conversation_switch(self):
        """
        Test that typing timers are properly cancelled when switching conversations.
        Reproduces issue #2.5 from audit report.
        """
        from unittest.mock import Mock
        
        # Mock Timer
        mock_timer = Mock()
        
        # Simulate provider state
        typing_timers = {
            "user_1": mock_timer,
            "user_2": mock_timer,
        }
        
        active_contact = "user_1"
        
        # Cancel timer for active contact
        if active_contact in typing_timers:
            typing_timers[active_contact].cancel()
            del typing_timers[active_contact]
        
        # Verify timer was cancelled
        assert mock_timer.cancel.called
        assert "user_1" not in typing_timers
        assert "user_2" in typing_timers  # Other timers preserved


class TestWebSocketReconnectionStorm:
    """Test WebSocket reconnection behavior"""

    def test_exponential_backoff_with_jitter(self):
        """
        Test that reconnection uses exponential backoff with sufficient jitter.
        Reproduces issue #2.6 from audit report.
        """
        import random
        
        initial_backoff = 2000  # 2 seconds in ms
        jitter_factor = 0.5
        
        delays = []
        for _ in range(100):
            backoff_ms = initial_backoff * (2 ** 0)  # First retry
            jitter_range = backoff_ms * jitter_factor
            jitter = (random.random() * 2 - 1) * jitter_range
            delay_ms = backoff_ms + jitter
            delays.append(delay_ms)
        
        # Jitter should create spread of Â±50%
        min_delay = min(delays)
        max_delay = max(delays)
        
        assert min_delay < initial_backoff * 0.6  # At least some below base
        assert max_delay > initial_backoff * 1.4  # At least some above base

    def test_initial_random_delay(self):
        """
        Test that first reconnect has random 0-5 second delay.
        """
        import random
        
        initial_delays = []
        for _ in range(100):
            initial_delay = random.randint(0, 5000)  # 0-5 seconds in ms
            initial_delays.append(initial_delay)
        
        # Should have good distribution
        assert min(initial_delays) < 1000  # Some < 1 second
        assert max(initial_delays) > 4000  # Some > 4 seconds


class TestMessageMergeLogic:
    """Test message cache merge logic"""

    def test_merge_with_invalid_ids(self):
        """
        Test that messages with invalid IDs are filtered out.
        Reproduces issue #2.3 from audit report.
        """
        # Simulate merge logic
        cached = [
            {"id": 1, "body": "msg1", "timestamp": "2024-01-01T00:00:00Z"},
            {"id": 0, "body": "invalid", "timestamp": "2024-01-01T00:00:00Z"},  # Invalid
            {"id": -1, "body": "invalid", "timestamp": "2024-01-01T00:00:00Z"},  # Invalid
        ]
        
        fresh = [
            {"id": 2, "body": "msg2", "timestamp": "2024-01-01T00:01:00Z"},
            {"id": 0, "body": "invalid", "timestamp": "2024-01-01T00:01:00Z"},  # Invalid
        ]
        
        merged = {}
        for msg in cached:
            if msg["id"] <= 0:
                continue
            merged[msg["id"]] = msg
        
        for msg in fresh:
            if msg["id"] <= 0:
                continue
            merged[msg["id"]] = msg
        
        # Should only have valid messages
        assert len(merged) == 2
        assert 1 in merged
        assert 2 in merged
        assert 0 not in merged
        assert -1 not in merged

    def test_merge_with_null_timestamps(self):
        """
        Test that null timestamps are handled gracefully.
        """
        messages = [
            {"id": 1, "timestamp": "2024-01-01T00:00:00Z"},
            {"id": 2, "timestamp": None},
            {"id": 3, "timestamp": "2024-01-01T00:02:00Z"},
        ]
        
        # Sort with null safety
        def safe_sort_key(msg):
            ts = msg.get("timestamp")
            if ts is None:
                return ""
            return ts
        
        messages.sort(key=safe_sort_key, reverse=True)
        
        # Should not crash, nulls should be at end
        assert messages[0]["id"] == 3
        assert messages[-1]["id"] == 2  # Null timestamp at end


class TestRetryLogic:
    """Test outbox message retry logic"""

    @pytest.mark.asyncio
    async def test_exponential_backoff_calculation(self):
        """
        Test retry delay calculation with exponential backoff.
        """
        from services.retry_service import MAX_RETRIES, BASE_DELAY_SECONDS, MAX_DELAY_SECONDS
        
        retry_count = 0
        delays = []
        
        while retry_count < MAX_RETRIES:
            delay = min(BASE_DELAY_SECONDS * (2 ** retry_count), MAX_DELAY_SECONDS)
            delays.append(delay)
            retry_count += 1
        
        # Should have exponential growth
        assert delays[0] == 60    # 1 minute
        assert delays[1] == 120   # 2 minutes
        assert delays[2] == 240   # 4 minutes
        assert delays[3] == 480   # 8 minutes
        assert delays[4] == 960   # 16 minutes

    @pytest.mark.asyncio
    async def test_max_retries_exceeded(self):
        """
        Test that messages are marked failed after max retries.
        """
        from services.retry_service import mark_message_for_retry, MAX_RETRIES
        
        mock_db_context = AsyncMock()
        mock_db = AsyncMock()
        mock_db.__aenter__ = AsyncMock(return_value=mock_db)
        mock_db.__aexit__ = AsyncMock()
        
        # Simulate message at max retries
        mock_fetch = AsyncMock(return_value={"retry_count": MAX_RETRIES})
        
        with patch("services.retry_service.get_db", return_value=mock_db_context):
            with patch("services.retry_service.fetch_one", side_effect=mock_fetch):
                with patch("services.retry_service.mark_outbox_failed") as mock_mark_failed:
                    result = await mark_message_for_retry(1, 123, "Test error")
                    
                    # Should return None (no more retries)
                    assert result is None
                    
                    # Should mark as failed
                    mock_mark_failed.assert_called()


# ============ Integration Tests ============

class TestConversationDeleteRateLimiting:
    """Test rate limiting on conversation delete endpoints"""

    @pytest.mark.asyncio
    async def test_delete_rate_limit_applied(self):
        """
        Test that delete endpoints have rate limiting.
        """
        # This would be tested via actual HTTP requests in integration testing
        # For now, verify the decorator is present
        from routes.chat_routes import delete_conversation_route
        
        # Check function has rate limiting decorator
        # (In real testing, would make 6 rapid requests and verify 429 on 6th)
        assert hasattr(delete_conversation_route, '__wrapped__')  # Has decorator


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-x"])
